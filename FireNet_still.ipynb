{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import os\n",
    "from scipy import misc\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.densenet import DenseNet121, decode_predictions\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense, Conv2D, Conv1D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation, Lambda\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Model, Input\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import keras\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Add\n",
    "\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras import backend as K\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Permute, multiply\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, channel = 224, 224, 3 # Resolution of inputs\n",
    "    \n",
    "#resnet = ResNet50(input_shape = (img_rows, img_cols, channel))\n",
    "#densenet = DenseNet121(include_top=False, \n",
    "#                     weights='imagenet', \n",
    "#                     input_tensor=None, \n",
    "#                     input_shape=(img_rows, img_cols, channel), \n",
    "#                     pooling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "\n",
    "#img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "img_rows, img_cols = 64, 64 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 2 \n",
    "batch_size = 16\n",
    "nb_epoch = 50\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FireNet(img_rows, img_cols, channel):\n",
    "    model_input = keras.layers.Input(shape=(img_rows, img_cols, channel))\n",
    "    \n",
    "    # entranse convolution part\n",
    "    entrance_featmap = Conv2D(filters = 16,\n",
    "                              kernel_size = (3, 3),\n",
    "                              activation='relu',\n",
    "                              strides = [1,1],\n",
    "                              input_shape = (img_rows, img_cols, channel),\n",
    "                              padding = 'same')(model_input)\n",
    "    \n",
    "    pooling_1 = AveragePooling2D(pool_size=(2, 2), strides=[2,2], padding='valid')(entrance_featmap)\n",
    "    pooling_1_drop = Dropout(0.5)(pooling_1)\n",
    "    \n",
    "    x_conv_2 = Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation='relu')(pooling_1_drop)\n",
    "    pooling_2 = AveragePooling2D(pool_size=(2, 2), strides=[2,2], padding='valid')(x_conv_2)\n",
    "    pooling_2_drop = Dropout(0.5)(pooling_2)\n",
    "    \n",
    "    x_conv_3 = Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation='relu')(pooling_2_drop)\n",
    "    pooling_3 = AveragePooling2D(pool_size=(2, 2), strides=[2,2], padding='valid')(x_conv_3)\n",
    "    pooling_3_drop = Dropout(0.5)(pooling_3)\n",
    "    \n",
    "    layer_4 = Flatten()(pooling_3_drop)\n",
    "    layer_4_dense = Dense(256, activation = 'relu')(layer_4)\n",
    "    layer_4_dense_drop = Dropout(0.2)(layer_4_dense)\n",
    "    \n",
    "    layer_5_dense = Dense(128, activation = 'relu')(layer_4_dense_drop)\n",
    "    output_ = Dense(2, activation = 'softmax')(layer_4_dense_drop)\n",
    "    print(output_)\n",
    "    \n",
    "    model = keras.models.Model(inputs = model_input, outputs = output_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atxap/anaconda3/envs/mot/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/atxap/anaconda3/envs/mot/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Tensor(\"dense_3/Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = FireNet(img_rows, img_cols, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,072,930\n",
      "Trainable params: 1,072,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "opt_adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = [1,1]\n",
    "        self.lr = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "def step_decay(epoch):\n",
    "    if epoch <= 10:\n",
    "        lrate = 1e-4\n",
    "    elif epoch > 10 and epoch <= 20:\n",
    "        lrate = 1e-5\n",
    "    elif epoch > 20 and epoch <= 30:\n",
    "        lrate = 2*1e-6\n",
    "    elif epoch > 30 and epoch <= 40:\n",
    "        lrate = 1e-6\n",
    "    elif epoch > 40:\n",
    "        lrate = 1e-7\n",
    "    else:\n",
    "         print('invalid')\n",
    "    return lrate\n",
    "\n",
    "mcp_save = ModelCheckpoint('./ckpt_folder/firenet_still.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')   \n",
    "history=LossHistory()\n",
    "lrate=LearningRateScheduler(step_decay)\n",
    "\n",
    "#lrate=LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load input data clear~!! =========================================\n",
      "load label data clear~!! =========================================\n",
      "load dataset clear~!! ============================================\n",
      "x_train_fismo: (3390, 224, 224, 3)\n",
      "x_test_fismo: (283, 224, 224, 3)\n",
      "y_train_fismo: (3390, 2)\n",
      "y_test_fismo: (283, 2)\n",
      "==================================================================\n",
      "x_train_bow: (1083, 224, 224, 3)\n",
      "x_test_bow: (91, 224, 224, 3)\n",
      "y_train_bow: (1083, 2)\n",
      "y_test_bow: (91, 2)\n",
      "==================================================================\n",
      "x_train_sharma: (3123, 224, 224, 3)\n",
      "x_test_sharma: (261, 224, 224, 3)\n",
      "y_train_sharma: (3123, 2)\n",
      "y_test_sharma: (261, 2)\n",
      "x_train: (12596, 224, 224, 3)\n",
      "x_test: (1885, 224, 224, 3)\n",
      "y_train: (12596, 2)\n",
      "y_test: (1885, 2)\n",
      "dataset shape check clear~!! =====================================\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "with open ('./datafiles_aug/x_train_fismo', 'rb') as fp:\n",
    "    x_train_fismo = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_train_bow', 'rb') as fp:\n",
    "    x_train_bow = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_train_sharma', 'rb') as fp:\n",
    "    x_train_sharma = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_test_fismo', 'rb') as fp:\n",
    "    x_test_fismo = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_test_bow', 'rb') as fp:\n",
    "    x_test_bow = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_test_sharma', 'rb') as fp:\n",
    "    x_test_sharma = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "with open ('./datafiles_aug/x_train_still.pickle', 'rb') as fp:\n",
    "    x_train_still = pickle.load(fp, encoding='latin1') \n",
    "\n",
    "with open ('./datafiles_aug/y_train_still.pickle', 'rb') as fp:\n",
    "    y_train_still = pickle.load(fp, encoding='latin1')   \n",
    "    \n",
    "with open ('./datafiles_aug/x_test_still.pickle', 'rb') as fp:\n",
    "    x_test_still = pickle.load(fp, encoding='latin1') \n",
    "\n",
    "with open ('./datafiles_aug/y_test_still.pickle', 'rb') as fp:\n",
    "    y_test_still = pickle.load(fp, encoding='latin1') \n",
    "    \n",
    "print('load input data clear~!! =========================================')\n",
    "\n",
    "with open ('./datafiles_aug/y_train_fismo', 'rb') as fp:\n",
    "    y_train_fismo = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/y_test_fismo', 'rb') as fp:\n",
    "    y_test_fismo = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/y_train_bow', 'rb') as fp:\n",
    "    y_train_bow = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/y_test_bow', 'rb') as fp:\n",
    "    y_test_bow = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/y_train_sharma', 'rb') as fp:\n",
    "    y_train_sharma = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/y_test_sharma', 'rb') as fp:\n",
    "    y_test_sharma = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "print('load label data clear~!! =========================================')\n",
    "print('load dataset clear~!! ============================================')\n",
    "    \n",
    "print('x_train_fismo: ' + str(np.shape(x_train_fismo)))\n",
    "print('x_test_fismo: '+ str(np.shape(x_test_fismo)))\n",
    "print('y_train_fismo: ' + str(np.shape(y_train_fismo)))\n",
    "print('y_test_fismo: ' + str(np.shape(y_test_fismo)))\n",
    "print('==================================================================')\n",
    "print('x_train_bow: ' + str(np.shape(x_train_bow)))\n",
    "print('x_test_bow: '+ str(np.shape(x_test_bow)))\n",
    "print('y_train_bow: ' + str(np.shape(y_train_bow)))\n",
    "print('y_test_bow: ' + str(np.shape(y_test_bow)))\n",
    "print('==================================================================')\n",
    "print('x_train_sharma: ' + str(np.shape(x_train_sharma)))\n",
    "print('x_test_sharma: '+ str(np.shape(x_test_sharma)))\n",
    "print('y_train_sharma: ' + str(np.shape(y_train_sharma)))\n",
    "print('y_test_sharma: ' + str(np.shape(y_test_sharma)))\n",
    "\n",
    "x_train = np.concatenate([x_train_fismo,x_train_bow,x_train_sharma, x_train_still]) \n",
    "x_test = np.concatenate([x_test_fismo,x_test_bow,x_test_sharma, x_test_still]) \n",
    "y_train = np.concatenate([y_train_fismo,y_train_bow,y_train_sharma, y_train_still])\n",
    "y_test = np.concatenate([y_test_fismo,y_test_bow,y_test_sharma, y_test_still])\n",
    "\n",
    "print('x_train: ' + str(np.shape(x_train)))\n",
    "print('x_test: '+ str(np.shape(x_test)))\n",
    "print('y_train: ' + str(np.shape(y_train)))\n",
    "print('y_test: ' + str(np.shape(y_test)))\n",
    "\n",
    "print('dataset shape check clear~!! =====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resized x_train data : (12596, 64, 64, 3)\n",
      "resized x_test data : (1885, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_resize = []\n",
    "x_test_resize = []\n",
    "\n",
    "img_number, _, _, _ = np.shape(x_train)\n",
    "for idx in range(img_number):\n",
    "    x_train_resize.append( resize(x_train[idx], (64, 64)) )\n",
    "    \n",
    "img_number, _, _, _ = np.shape(x_test)\n",
    "for idx in range(img_number):\n",
    "    x_test_resize.append( resize(x_test[idx], (64, 64)) )\n",
    "\n",
    "x_train_resize = np.array(x_train_resize)/255.\n",
    "x_test_resize = np.array(x_test_resize)/255.\n",
    "\n",
    "print(\"resized x_train data : \" + str(np.shape(x_train_resize)) )\n",
    "print(\"resized x_test data : \" + str(np.shape(x_test_resize)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atxap/anaconda3/envs/mot/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 12596 samples, validate on 1885 samples\n",
      "Epoch 1/50\n",
      "12596/12596 [==============================] - 12s 944us/step - loss: 0.5955 - accuracy: 0.6944 - val_loss: 0.5436 - val_accuracy: 0.7708\n",
      "Epoch 2/50\n",
      "12596/12596 [==============================] - 10s 787us/step - loss: 0.5195 - accuracy: 0.7556 - val_loss: 0.5491 - val_accuracy: 0.7342\n",
      "Epoch 3/50\n",
      "12596/12596 [==============================] - 10s 810us/step - loss: 0.4769 - accuracy: 0.7849 - val_loss: 0.5062 - val_accuracy: 0.7698\n",
      "Epoch 4/50\n",
      "12596/12596 [==============================] - 11s 850us/step - loss: 0.4407 - accuracy: 0.8052 - val_loss: 0.4507 - val_accuracy: 0.8021\n",
      "Epoch 5/50\n",
      "12596/12596 [==============================] - 10s 808us/step - loss: 0.4106 - accuracy: 0.8195 - val_loss: 0.4195 - val_accuracy: 0.8186\n",
      "Epoch 6/50\n",
      "12596/12596 [==============================] - 10s 819us/step - loss: 0.3869 - accuracy: 0.8322 - val_loss: 0.3924 - val_accuracy: 0.8424\n",
      "Epoch 7/50\n",
      "12596/12596 [==============================] - 10s 799us/step - loss: 0.3661 - accuracy: 0.8465 - val_loss: 0.3886 - val_accuracy: 0.8520\n",
      "Epoch 8/50\n",
      "12596/12596 [==============================] - 10s 808us/step - loss: 0.3461 - accuracy: 0.8595 - val_loss: 0.3781 - val_accuracy: 0.8440\n",
      "Epoch 9/50\n",
      "12596/12596 [==============================] - 11s 841us/step - loss: 0.3310 - accuracy: 0.8623 - val_loss: 0.3772 - val_accuracy: 0.8584\n",
      "Epoch 10/50\n",
      "12596/12596 [==============================] - 10s 823us/step - loss: 0.3154 - accuracy: 0.8726 - val_loss: 0.3583 - val_accuracy: 0.8631\n",
      "Epoch 11/50\n",
      "12596/12596 [==============================] - 10s 759us/step - loss: 0.3034 - accuracy: 0.8765 - val_loss: 0.3852 - val_accuracy: 0.8419\n",
      "Epoch 12/50\n",
      "12596/12596 [==============================] - 11s 841us/step - loss: 0.2910 - accuracy: 0.8786 - val_loss: 0.3611 - val_accuracy: 0.8515\n",
      "Epoch 13/50\n",
      "12596/12596 [==============================] - 10s 812us/step - loss: 0.2837 - accuracy: 0.8882 - val_loss: 0.3516 - val_accuracy: 0.8589\n",
      "Epoch 14/50\n",
      "12596/12596 [==============================] - 10s 829us/step - loss: 0.2734 - accuracy: 0.8922 - val_loss: 0.3554 - val_accuracy: 0.8573\n",
      "Epoch 15/50\n",
      "12596/12596 [==============================] - 11s 838us/step - loss: 0.2630 - accuracy: 0.8993 - val_loss: 0.3303 - val_accuracy: 0.8748\n",
      "Epoch 16/50\n",
      "12596/12596 [==============================] - 10s 817us/step - loss: 0.2560 - accuracy: 0.9005 - val_loss: 0.3438 - val_accuracy: 0.8663\n",
      "Epoch 17/50\n",
      "12596/12596 [==============================] - 9s 718us/step - loss: 0.2474 - accuracy: 0.9039 - val_loss: 0.3274 - val_accuracy: 0.8743\n",
      "Epoch 18/50\n",
      "12596/12596 [==============================] - 11s 845us/step - loss: 0.2335 - accuracy: 0.9085 - val_loss: 0.3582 - val_accuracy: 0.8674\n",
      "Epoch 19/50\n",
      "12596/12596 [==============================] - 10s 825us/step - loss: 0.2295 - accuracy: 0.9101 - val_loss: 0.3379 - val_accuracy: 0.8796\n",
      "Epoch 20/50\n",
      "12596/12596 [==============================] - 10s 805us/step - loss: 0.2259 - accuracy: 0.9102 - val_loss: 0.3386 - val_accuracy: 0.8695\n",
      "Epoch 21/50\n",
      "12596/12596 [==============================] - 10s 808us/step - loss: 0.2160 - accuracy: 0.9154 - val_loss: 0.3241 - val_accuracy: 0.8759\n",
      "Epoch 22/50\n",
      "12596/12596 [==============================] - 9s 740us/step - loss: 0.2173 - accuracy: 0.9158 - val_loss: 0.3202 - val_accuracy: 0.8790\n",
      "Epoch 23/50\n",
      "12596/12596 [==============================] - 11s 845us/step - loss: 0.2043 - accuracy: 0.9238 - val_loss: 0.3234 - val_accuracy: 0.8775\n",
      "Epoch 24/50\n",
      "12596/12596 [==============================] - 11s 839us/step - loss: 0.1981 - accuracy: 0.9252 - val_loss: 0.3020 - val_accuracy: 0.8854\n",
      "Epoch 25/50\n",
      "12596/12596 [==============================] - 9s 751us/step - loss: 0.1911 - accuracy: 0.9273 - val_loss: 0.3343 - val_accuracy: 0.8838\n",
      "Epoch 26/50\n",
      "12596/12596 [==============================] - 10s 817us/step - loss: 0.1918 - accuracy: 0.9285 - val_loss: 0.3059 - val_accuracy: 0.8801\n",
      "Epoch 27/50\n",
      "12596/12596 [==============================] - 11s 850us/step - loss: 0.1827 - accuracy: 0.9320 - val_loss: 0.3439 - val_accuracy: 0.8653\n",
      "Epoch 28/50\n",
      "12596/12596 [==============================] - 11s 840us/step - loss: 0.1832 - accuracy: 0.9310 - val_loss: 0.3145 - val_accuracy: 0.8875\n",
      "Epoch 29/50\n",
      "12596/12596 [==============================] - 10s 795us/step - loss: 0.1714 - accuracy: 0.9355 - val_loss: 0.3198 - val_accuracy: 0.8833\n",
      "Epoch 30/50\n",
      "12596/12596 [==============================] - 11s 848us/step - loss: 0.1698 - accuracy: 0.9371 - val_loss: 0.3377 - val_accuracy: 0.8801\n",
      "Epoch 31/50\n",
      "12596/12596 [==============================] - 10s 795us/step - loss: 0.1663 - accuracy: 0.9400 - val_loss: 0.3201 - val_accuracy: 0.8881\n",
      "Epoch 32/50\n",
      "12596/12596 [==============================] - 10s 814us/step - loss: 0.1602 - accuracy: 0.9405 - val_loss: 0.3227 - val_accuracy: 0.8891\n",
      "Epoch 33/50\n",
      "12596/12596 [==============================] - 10s 796us/step - loss: 0.1573 - accuracy: 0.9430 - val_loss: 0.3403 - val_accuracy: 0.8806\n",
      "Epoch 34/50\n",
      "12596/12596 [==============================] - 11s 848us/step - loss: 0.1497 - accuracy: 0.9414 - val_loss: 0.3360 - val_accuracy: 0.8886\n",
      "Epoch 35/50\n",
      "12596/12596 [==============================] - 10s 788us/step - loss: 0.1498 - accuracy: 0.9450 - val_loss: 0.3090 - val_accuracy: 0.8828\n",
      "Epoch 36/50\n",
      "12596/12596 [==============================] - 11s 843us/step - loss: 0.1455 - accuracy: 0.9485 - val_loss: 0.2953 - val_accuracy: 0.8918\n",
      "Epoch 37/50\n",
      "12596/12596 [==============================] - 10s 808us/step - loss: 0.1411 - accuracy: 0.9484 - val_loss: 0.3196 - val_accuracy: 0.8881\n",
      "Epoch 38/50\n",
      "12596/12596 [==============================] - 10s 796us/step - loss: 0.1401 - accuracy: 0.9475 - val_loss: 0.3264 - val_accuracy: 0.8875\n",
      "Epoch 39/50\n",
      "12596/12596 [==============================] - 11s 849us/step - loss: 0.1308 - accuracy: 0.9527 - val_loss: 0.3304 - val_accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "12596/12596 [==============================] - 11s 843us/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.3256 - val_accuracy: 0.8987\n",
      "Epoch 41/50\n",
      "12596/12596 [==============================] - 10s 794us/step - loss: 0.1283 - accuracy: 0.9518 - val_loss: 0.3167 - val_accuracy: 0.8854\n",
      "Epoch 42/50\n",
      "12596/12596 [==============================] - 11s 848us/step - loss: 0.1213 - accuracy: 0.9563 - val_loss: 0.3329 - val_accuracy: 0.8907\n",
      "Epoch 43/50\n",
      "12596/12596 [==============================] - 10s 820us/step - loss: 0.1228 - accuracy: 0.9567 - val_loss: 0.3013 - val_accuracy: 0.8971\n",
      "Epoch 44/50\n",
      "12596/12596 [==============================] - 10s 776us/step - loss: 0.1176 - accuracy: 0.9538 - val_loss: 0.3129 - val_accuracy: 0.8928\n",
      "Epoch 45/50\n",
      "12596/12596 [==============================] - 10s 833us/step - loss: 0.1118 - accuracy: 0.9585 - val_loss: 0.3142 - val_accuracy: 0.8960\n",
      "Epoch 46/50\n",
      "12596/12596 [==============================] - 10s 820us/step - loss: 0.1117 - accuracy: 0.9597 - val_loss: 0.3257 - val_accuracy: 0.8907\n",
      "Epoch 47/50\n",
      "12596/12596 [==============================] - 11s 842us/step - loss: 0.1104 - accuracy: 0.9613 - val_loss: 0.3185 - val_accuracy: 0.8981\n",
      "Epoch 48/50\n",
      "12596/12596 [==============================] - 10s 793us/step - loss: 0.1065 - accuracy: 0.9609 - val_loss: 0.3038 - val_accuracy: 0.8997\n",
      "Epoch 49/50\n",
      "12596/12596 [==============================] - 10s 768us/step - loss: 0.0988 - accuracy: 0.9636 - val_loss: 0.3110 - val_accuracy: 0.8971\n",
      "Epoch 50/50\n",
      "12596/12596 [==============================] - 11s 848us/step - loss: 0.0998 - accuracy: 0.9636 - val_loss: 0.3204 - val_accuracy: 0.8944\n",
      "1885/1885 [==============================] - 0s 209us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt_adam, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_resize, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test_resize, y_test),\n",
    "                #callbacks=[lrate])\n",
    "                callbacks=[history, mcp_save])\n",
    "\n",
    "predictions_valid = model.predict(x_test_resize, batch_size=batch_size, verbose=1)\n",
    "# Combine 3 set of outputs using averaging\n",
    "predictions_valid = sum(predictions_valid)/len(predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here, test part work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = DCR_block_classification(img_rows, img_cols, channel)\n",
    "loaded_model = model\n",
    "loaded_model.load_weights('./ckpt_folder/firenet_still.hdf5')\n",
    "opt_adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "loaded_model.compile(optimizer=opt_adam, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,072,930\n",
      "Trainable params: 1,072,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load input data clear~!! =========================================\n",
      "load label data clear~!! =========================================\n",
      "load dataset clear~!! ============================================\n",
      "x_test_fismo: (283, 224, 224, 3)\n",
      "y_test_fismo: (283, 2)\n",
      "==================================================================\n",
      "x_test_bow: (91, 224, 224, 3)\n",
      "y_test_bow: (91, 2)\n",
      "==================================================================\n",
      "x_test_sharma: (261, 224, 224, 3)\n",
      "y_test_sharma: (261, 2)\n",
      "x_test_still: (1250, 224, 224, 3)\n",
      "y_test_still: (1250, 2)\n",
      "x_test: (1885, 224, 224, 3)\n",
      "y_test: (1885, 2)\n",
      "dataset shape check clear~!! =====================================\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "with open ('./datafiles_aug/x_test_fismo', 'rb') as fp:\n",
    "    x_test_fismo = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_test_bow', 'rb') as fp:\n",
    "    x_test_bow = pickle.load(fp, encoding='latin1')\n",
    "with open ('./datafiles_aug/x_test_sharma', 'rb') as fp:\n",
    "    x_test_sharma = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "print('load input data clear~!! =========================================')\n",
    "\n",
    "\n",
    "with open ('./datafiles_aug/y_test_fismo', 'rb') as fp:\n",
    "    y_test_fismo = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "with open ('./datafiles_aug/y_test_bow', 'rb') as fp:\n",
    "    y_test_bow = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "with open ('./datafiles_aug/y_test_sharma', 'rb') as fp:\n",
    "    y_test_sharma = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "# with open ('./datafiles_aug/x_train_still.pickle', 'rb') as fp:\n",
    "#     x_train_still = pickle.load(fp, encoding='latin1')   \n",
    "\n",
    "# with open ('./datafiles_aug/y_train_still.pickle', 'rb') as fp:\n",
    "#     y_train_still = pickle.load(fp, encoding='latin1')   \n",
    "    \n",
    "with open ('./datafiles_aug/x_test_still.pickle', 'rb') as fp:\n",
    "    x_test_still = pickle.load(fp, encoding='latin1') \n",
    "\n",
    "with open ('./datafiles_aug/y_test_still.pickle', 'rb') as fp:\n",
    "    y_test_still = pickle.load(fp, encoding='latin1') \n",
    "    \n",
    "    \n",
    "    \n",
    "print('load label data clear~!! =========================================')\n",
    "print('load dataset clear~!! ============================================')\n",
    "    \n",
    "\n",
    "print('x_test_fismo: '+ str(np.shape(x_test_fismo)))\n",
    "\n",
    "print('y_test_fismo: ' + str(np.shape(y_test_fismo)))\n",
    "print('==================================================================')\n",
    "\n",
    "print('x_test_bow: '+ str(np.shape(x_test_bow)))\n",
    "\n",
    "print('y_test_bow: ' + str(np.shape(y_test_bow)))\n",
    "print('==================================================================')\n",
    "\n",
    "print('x_test_sharma: '+ str(np.shape(x_test_sharma)))\n",
    "\n",
    "print('y_test_sharma: ' + str(np.shape(y_test_sharma)))\n",
    "\n",
    "# print('x_train_still: '+ str(np.shape(x_train_still)))\n",
    "\n",
    "# print('y_train_still: ' + str(np.shape(y_train_still)))\n",
    "\n",
    "print('x_test_still: '+ str(np.shape(x_test_still)))\n",
    "\n",
    "print('y_test_still: ' + str(np.shape(y_test_still)))\n",
    "\n",
    "\n",
    "x_test = np.concatenate([x_test_fismo,x_test_bow,x_test_sharma,x_test_still]) / 255.\n",
    "y_test = np.concatenate([y_test_fismo,y_test_bow,y_test_sharma,y_test_still])\n",
    "\n",
    "\n",
    "# x_still = np.concatenate([x_train_still, x_test_still]) / 255.\n",
    "# y_still = np.concatenate([y_train_still, y_test_still])\n",
    "\n",
    "# print('x_train: ' + str(np.shape(x_train)))\n",
    "print('x_test: '+ str(np.shape(x_test)))\n",
    "# print('y_train: ' + str(np.shape(y_train)))\n",
    "print('y_test: ' + str(np.shape(y_test)))\n",
    "\n",
    "print('dataset shape check clear~!! =====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fismo_resize = []\n",
    "bow_resize = []\n",
    "sharma_resize = []\n",
    "still_resize =[]\n",
    "\n",
    "img_number, _, _, _ = np.shape(x_test_fismo)\n",
    "for idx in range(img_number):\n",
    "    fismo_resize.append( resize(x_test_fismo[idx], (64, 64)) )\n",
    "    \n",
    "img_number, _, _, _ = np.shape(x_test_bow)\n",
    "for idx in range(img_number):\n",
    "    bow_resize.append( resize(x_test_bow[idx], (64, 64)) )\n",
    "    \n",
    "img_number, _, _, _ = np.shape(x_test_sharma)\n",
    "for idx in range(img_number):\n",
    "    sharma_resize.append( resize(x_test_sharma[idx], (64, 64)) )\n",
    "    \n",
    "img_number, _, _, _ = np.shape(x_test_still)\n",
    "for idx in range(img_number):\n",
    "    still_resize.append( resize(x_test_still[idx], (64, 64)) )\n",
    "\n",
    "fismo_resize = np.array(fismo_resize) /255.\n",
    "bow_resize = np.array(bow_resize) /255.\n",
    "sharma_resize = np.array(sharma_resize) /255. \n",
    "still_resize = np.array(still_resize) /255. \n",
    "\n",
    "x_test_resize = np.concatenate([fismo_resize,bow_resize,sharma_resize, still_resize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atxap/anaconda3/envs/mot/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "283/283 [==============================] - 1s 4ms/step\n",
      "\n",
      "loss_and_metrics_fismo : [0.21747631003073165, 0.9434629082679749]\n",
      "91/91 [==============================] - 0s 440us/step\n",
      "\n",
      "loss_and_metrics_bow : [0.5775536894798279, 0.8461538553237915]\n",
      "261/261 [==============================] - 0s 137us/step\n",
      "\n",
      "loss_and_metrics_sharma : [0.16772477298803712, 0.938697338104248]\n",
      "1250/1250 [==============================] - 0s 69us/step\n",
      "\n",
      "loss_and_metrics_still : [0.3317923585653305, 0.8855999708175659]\n",
      "1885/1885 [==============================] - 0s 70us/step\n",
      "\n",
      "loss_and_metrics_entire : [0.3037770777861699, 0.8997347354888916]\n",
      "\n",
      "Accuracy\n",
      "0.899735 & 0.943463 & 0.846154 & 0.938697 & 0.8856\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = loaded_model.evaluate(fismo_resize, np.array(y_test_fismo), batch_size=128)\n",
    "print('')\n",
    "print('loss_and_metrics_fismo : ' + str(loss_and_metrics))\n",
    "fismo = round(loss_and_metrics[1],6)\n",
    "\n",
    "\n",
    "loss_and_metrics = loaded_model.evaluate(bow_resize, np.array(y_test_bow), batch_size=128)\n",
    "print('')\n",
    "print('loss_and_metrics_bow : ' + str(loss_and_metrics))\n",
    "bow = round(loss_and_metrics[1],6)\n",
    "\n",
    "\n",
    "loss_and_metrics = loaded_model.evaluate(sharma_resize, np.array(y_test_sharma), batch_size=128)\n",
    "print('')\n",
    "print('loss_and_metrics_sharma : ' + str(loss_and_metrics))\n",
    "sharma = round(loss_and_metrics[1],6)\n",
    "\n",
    "\n",
    "\n",
    "loss_and_metrics = loaded_model.evaluate(still_resize, np.array(y_test_still), batch_size=128)\n",
    "print('')\n",
    "print('loss_and_metrics_still : ' + str(loss_and_metrics))\n",
    "still = round(loss_and_metrics[1],6)\n",
    "\n",
    "\n",
    "loss_and_metrics = loaded_model.evaluate(x_test_resize, np.array(y_test), batch_size=128)\n",
    "print('')\n",
    "print('loss_and_metrics_entire : ' + str(loss_and_metrics))\n",
    "entire = round(loss_and_metrics[1],6)\n",
    "\n",
    "print('')\n",
    "print ('Accuracy')\n",
    "print (entire, '&', fismo, '&',bow, '&',sharma, '&',still)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885/1885 [==============================] - 0s 71us/step\n",
      "283/283 [==============================] - 0s 58us/step\n",
      "91/91 [==============================] - 0s 65us/step\n",
      "261/261 [==============================] - 0s 77us/step\n",
      "1250/1250 [==============================] - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "predictions_entire = loaded_model.predict(x_test_resize, batch_size=128, verbose=1)\n",
    "predictions_fismo = loaded_model.predict(fismo_resize, batch_size=128, verbose=1)\n",
    "predictions_bow = loaded_model.predict(bow_resize, batch_size=128, verbose=1)\n",
    "predictions_sharma = loaded_model.predict(sharma_resize, batch_size=128, verbose=1)\n",
    "predictions_still = loaded_model.predict(still_resize, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_result(predictions_valid):\n",
    "    result_list = []\n",
    "    for i in range(len(predictions_valid)):\n",
    "        if (predictions_valid[i][0] > predictions_valid[i][1]):\n",
    "            result_list.append(0)\n",
    "        else : result_list.append(1)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_entire_tmp = prediction_result(predictions_entire)\n",
    "y_test_tmp = prediction_result(y_test)\n",
    "predictions_fismo_tmp = prediction_result(predictions_fismo)\n",
    "y_test_fismo_tmp = prediction_result(y_test_fismo)\n",
    "predictions_bow_tmp = prediction_result(predictions_bow)\n",
    "y_test_bow_tmp = prediction_result(y_test_bow)\n",
    "predictions_sharma_tmp = prediction_result(predictions_sharma)\n",
    "y_test_sharma_tmp = prediction_result(y_test_sharma)\n",
    "predictions_still_tmp = prediction_result(predictions_still)\n",
    "y_test_still_tmp = prediction_result(y_test_still)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION\n",
      "0.8772504091653028\n",
      "0.9692307692307692\n",
      "0.9056603773584906\n",
      "0.8571428571428571\n",
      "0.8184713375796179\n",
      "\n",
      "0.87725 & 0.969231 & 0.90566 & 0.857143 & 0.818471\n",
      "\n",
      "RECALL\n",
      "0.8246153846153846\n",
      "0.949748743718593\n",
      "0.8421052631578947\n",
      "0.8235294117647058\n",
      "0.749271137026239\n",
      "\n",
      "0.824615 & 0.949749 & 0.842105 & 0.823529 & 0.749271\n",
      "\n",
      "F1SCORE\n",
      "0.8501189532117368\n",
      "0.9593908629441623\n",
      "0.8727272727272727\n",
      "0.84\n",
      "0.78234398782344\n",
      "\n",
      "0.850119 & 0.959391 & 0.872727 & 0.84 & 0.782344\n"
     ]
    }
   ],
   "source": [
    "print('PRECISION')\n",
    "entire= sm.precision_score(y_test_tmp, predictions_entire_tmp)\n",
    "fismo = sm.precision_score(y_test_fismo_tmp, predictions_fismo_tmp)\n",
    "bow = sm.precision_score(y_test_bow_tmp, predictions_bow_tmp)\n",
    "sharma = sm.precision_score(y_test_sharma_tmp, predictions_sharma_tmp)\n",
    "still = sm.precision_score(y_test_still_tmp, predictions_still_tmp)\n",
    "print  (entire)\n",
    "print  (fismo)\n",
    "print  (bow)\n",
    "print  (sharma)\n",
    "# print  (sm.precision_score(y_train_still_tmp, predictions_still_train_tmp))\n",
    "print  (still)\n",
    "\n",
    "print('')\n",
    "print (round(entire,6), '&', round(fismo,6), '&',round(bow,6), '&',round(sharma,6), '&',round(still,6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entire= sm.recall_score(y_test_tmp, predictions_entire_tmp)\n",
    "fismo = sm.recall_score(y_test_fismo_tmp, predictions_fismo_tmp)\n",
    "bow = sm.recall_score(y_test_bow_tmp, predictions_bow_tmp)\n",
    "sharma = sm.recall_score(y_test_sharma_tmp, predictions_sharma_tmp)\n",
    "still = sm.recall_score(y_test_still_tmp, predictions_still_tmp)\n",
    "\n",
    "print('')\n",
    "print('RECALL')\n",
    "print  (entire)\n",
    "print  (fismo)\n",
    "print  (bow)\n",
    "print  (sharma)\n",
    "# print  (sm.precision_score(y_train_still_tmp, predictions_still_train_tmp))\n",
    "print  (still)\n",
    "\n",
    "print('')\n",
    "print (round(entire,6), '&', round(fismo,6), '&',round(bow,6), '&',round(sharma,6), '&',round(still,6))\n",
    "\n",
    "\n",
    "\n",
    "entire= sm.f1_score(y_test_tmp, predictions_entire_tmp)\n",
    "fismo = sm.f1_score(y_test_fismo_tmp, predictions_fismo_tmp)\n",
    "bow = sm.f1_score(y_test_bow_tmp, predictions_bow_tmp)\n",
    "sharma = sm.f1_score(y_test_sharma_tmp, predictions_sharma_tmp)\n",
    "still = sm.f1_score(y_test_still_tmp, predictions_still_tmp)\n",
    "\n",
    "print('')\n",
    "print('F1SCORE')\n",
    "print  (entire)\n",
    "print  (fismo)\n",
    "print  (bow)\n",
    "print  (sharma)\n",
    "# print  (sm.precision_score(y_train_still_tmp, predictions_still_train_tmp))\n",
    "print  (still)\n",
    "\n",
    "print('')\n",
    "print (round(entire,6), '&', round(fismo,6), '&',round(bow,6), '&',round(sharma,6), '&',round(still,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all :\n",
      "[[1160   75]\n",
      " [ 114  536]]\n",
      "fismo :\n",
      "[[ 78   6]\n",
      " [ 10 189]]\n",
      "bow :\n",
      "[[29  5]\n",
      " [ 9 48]]\n",
      "sharma :\n",
      "[[203   7]\n",
      " [  9  42]]\n",
      "still :\n",
      "[[850  57]\n",
      " [ 86 257]]\n"
     ]
    }
   ],
   "source": [
    "print ('all :')\n",
    "print (sm.confusion_matrix(y_test_tmp, predictions_entire_tmp))\n",
    "print ('fismo :')\n",
    "print (sm.confusion_matrix(y_test_fismo_tmp, predictions_fismo_tmp))\n",
    "print ('bow :')\n",
    "print (sm.confusion_matrix(y_test_bow_tmp, predictions_bow_tmp))\n",
    "print ('sharma :')\n",
    "print (sm.confusion_matrix(y_test_sharma_tmp, predictions_sharma_tmp))\n",
    "# print ('still_train :')\n",
    "# print (sm.confusion_matrix(y_train_still_tmp, predictions_still_train_tmp))\n",
    "print ('still :')\n",
    "print (sm.confusion_matrix(y_test_still_tmp, predictions_still_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR value - all : 0.06072874\n",
      "FAR value - fismo : 0.07142857\n",
      "FAR value - bow : 0.14705882\n",
      "FAR value - sharma : 0.03333333\n",
      "FAR value - still : 0.06284454\n",
      "\n",
      "FAR\n",
      "6.072874 & 7.142857 & 14.705882 & 3.333333 & 6.284454\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp =  sm.confusion_matrix(y_test_tmp, predictions_entire_tmp).ravel()\n",
    "\n",
    "entire_FAR = float(fp)/(tn+fp)\n",
    "print(\"FAR value - all : %1.8f\"%entire_FAR)\n",
    "\n",
    "tn, fp, fn, tp =  sm.confusion_matrix(y_test_fismo_tmp, predictions_fismo_tmp).ravel()\n",
    "\n",
    "fismo_FAR = float(fp)/(tn+fp)\n",
    "print(\"FAR value - fismo : %1.8f\"%fismo_FAR)\n",
    "\n",
    "tn, fp, fn, tp =  sm.confusion_matrix(y_test_bow_tmp, predictions_bow_tmp).ravel()\n",
    "\n",
    "bow_FAR = float(fp)/(tn+fp)\n",
    "print(\"FAR value - bow : %1.8f\"%bow_FAR)\n",
    "\n",
    "tn, fp, fn, tp =  sm.confusion_matrix(y_test_sharma_tmp, predictions_sharma_tmp).ravel()\n",
    "sharma_FAR = float(fp)/(tn+fp)\n",
    "print(\"FAR value - sharma : %1.8f\"%sharma_FAR)\n",
    "\n",
    "tn, fp, fn, tp =  sm.confusion_matrix(y_test_still_tmp, predictions_still_tmp).ravel()\n",
    "still_FAR = float(fp)/(tn+fp)\n",
    "print(\"FAR value - still : %1.8f\"%still_FAR)\n",
    "\n",
    "\n",
    "print('')\n",
    "print ('FAR')\n",
    "print (round(entire_FAR*100,6), '&', round(fismo_FAR*100,6), '&',round(bow_FAR*100,6), '&',round(sharma_FAR*100,6), '&',round(still_FAR*100,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
